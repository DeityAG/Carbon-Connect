{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas\n!pip install numpy\n!pip install sentence-transformers\n!pip install transformers\n!pip install scikit-learn\n!pip install python-docx\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T23:48:24.545949Z","iopub.execute_input":"2024-10-05T23:48:24.546389Z","iopub.status.idle":"2024-10-05T23:49:49.765056Z","shell.execute_reply.started":"2024-10-05T23:48:24.546351Z","shell.execute_reply":"2024-10-05T23:49:49.763350Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (3.1.1)\nRequirement already satisfied: transformers<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0+cpu)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.19.3->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: python-docx in /opt/conda/lib/python3.10/site-packages (1.1.2)\nRequirement already satisfied: lxml>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from python-docx) (5.3.0)\nRequirement already satisfied: typing-extensions>=4.9.0 in /opt/conda/lib/python3.10/site-packages (from python-docx) (4.12.2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Using GPT2 model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import pipeline\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom docx import Document\nimport re\n\nclass CarbonCreditDocGenerator:\n    def __init__(self):\n        self.sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.nlg_pipeline = pipeline(\"text-generation\", model=\"gpt2\", max_length=1000)\n        \n        # Load your knowledge base here\n        self.knowledge_base = self.load_knowledge_base()\n        \n    def load_knowledge_base(self):\n        # Expanded knowledge base\n        return [\n            \"Carbon credits represent the reduction of one metric ton of carbon dioxide emissions.\",\n            \"Afforestation projects involve planting trees in areas where there were none before.\",\n            \"The Verified Carbon Standard (VCS) is a widely recognized certification for carbon credits.\",\n            \"Carbon credit projects must demonstrate additionality, meaning the reductions wouldn't have occurred without the project.\",\n            \"Monitoring, reporting, and verification (MRV) are crucial components of carbon credit projects.\",\n            \"Project developers must provide detailed information about project location, type, and expected carbon sequestration.\",\n            \"Carbon credit pricing can vary based on project type, location, and additional benefits.\",\n            \"Environmental Impact Assessments (EIA) are often required for carbon credit projects.\",\n            \"Community engagement and social benefits are important aspects of many carbon credit projects.\",\n            \"Risk assessment and mitigation strategies are crucial for project success and credibility.\"\n        ]\n        \n    def process_input_data(self, input_text):\n        # Improved parsing of the input document\n        sections = re.split(r'\\d+\\.\\s+', input_text)[1:]  # Split by numbered sections\n        data = {}\n        current_section = \"\"\n        for section in sections:\n            lines = section.strip().split('\\n')\n            section_title = lines[0].strip()\n            current_section = section_title\n            data[current_section] = {}\n            for line in lines[1:]:\n                if ':' in line:\n                    key, value = line.split(':', 1)\n                    data[current_section][key.strip()] = value.strip()\n                else:\n                    # Append to the last key if no colon is found\n                    if data[current_section]:\n                        last_key = list(data[current_section].keys())[-1]\n                        data[current_section][last_key] += \" \" + line.strip()\n        return data\n    \n    def retrieve_relevant_knowledge(self, query, top_k=3):\n        query_embedding = self.sbert_model.encode([query])[0]\n        knowledge_embeddings = self.sbert_model.encode(self.knowledge_base)\n        \n        similarities = cosine_similarity([query_embedding], knowledge_embeddings)[0]\n        top_indices = np.argsort(similarities)[-top_k:][::-1]\n        \n        return [self.knowledge_base[i] for i in top_indices]\n    \n    def generate_section_content(self, section_title, input_data, max_length=1000):\n        query = f\"Generate content for the '{section_title}' section of a carbon credit document.\"\n        relevant_knowledge = self.retrieve_relevant_knowledge(query)\n        \n        # Use the specific section data if available, otherwise use all input data\n        section_data = input_data.get(section_title, input_data)\n        context = f\"Input data: {section_data}\\n\\nRelevant knowledge: {' '.join(relevant_knowledge)}\"\n        prompt = f\"{context}\\n\\nTask: {query}\\n\\nContent:\"\n        \n        generated_text = self.nlg_pipeline(prompt, max_length=max_length, num_return_sequences=1)[0]['generated_text']\n        \n        # Apply corrective RAG\n        corrected_text = self.apply_corrective_rag(generated_text, section_data, relevant_knowledge)\n        \n        return corrected_text\n    \n    def apply_corrective_rag(self, generated_text, input_data, relevant_knowledge):\n        corrected_text = generated_text\n        \n        # Ensure all input data is represented\n        for key, value in input_data.items():\n            if isinstance(value, dict):\n                for sub_key, sub_value in value.items():\n                    if sub_value.lower() not in corrected_text.lower():\n                        corrected_text += f\" {sub_key}: {sub_value}.\"\n            elif value.lower() not in corrected_text.lower():\n                corrected_text += f\" {key}: {value}.\"\n        \n        # Ensure relevant knowledge is incorporated\n        for knowledge in relevant_knowledge:\n            if knowledge.lower() not in corrected_text.lower():\n                corrected_text += f\" {knowledge}\"\n        \n        return corrected_text\n    \n    def create_document(self, input_text):\n        doc = Document()\n        doc.add_heading('Carbon Credit Project Document', 0)\n        \n        input_data = self.process_input_data(input_text)\n        \n        sections = [\n            \"Project Overview\",\n            \"Seller/Proponent Information\",\n            \"Carbon Credit Specifications\",\n            \"Financial & Pricing Information\",\n            \"Project Impact and Sustainability\",\n            \"Risks & Mitigation Strategies\",\n            \"Supporting Documentation\",\n            \"Declarations and Acknowledgements\"\n        ]\n        \n        for section in sections:\n            doc.add_heading(section, level=1)\n            content = self.generate_section_content(section, input_data)\n            doc.add_paragraph(content)\n        \n        return doc\n    \n    def generate_document(self, input_text, output_path):\n        doc = self.create_document(input_text)\n        doc.save(output_path)\n        print(f\"Carbon credit document generated and saved to {output_path}\")\n\n# Usage\ngenerator = CarbonCreditDocGenerator()\ninput_text = \"\"\"\nCarbon Credit Project Submission Form\n1. Project Overview\nProject Title: Green Future Reforestation Initiative\nExecutive Summary:\nThe Green Future Reforestation Initiative aims to restore 500 hectares of degraded land through afforestation and sustainable land management practices. The project seeks to sequester carbon dioxide, enhance biodiversity, and improve local air quality while providing economic opportunities for surrounding communities.\nLocation:\nCountry: Brazil\nRegion: Amazon Rainforest\nCoordinates: Latitude -3.4653, Longitude -62.2159\nProject Category: Afforestation\nProject Start Date: 01/15/2024\nExpected Completion Date: 12/31/2028\n2. Seller/Proponent Information\nOrganization Name: EcoSustainability Inc.\nContact Person: Maria Silva\nJob Title: Project Manager\nEmail Address: maria.silva@ecosustainability.org\nPhone Number: +55 11 98765-4321\nMailing Address:\nEcoSustainability Inc.\nAvenida Paulista, 1000\nSão Paulo, SP, Brazil\nZip Code: 01310-100\n3. Carbon Credit Specifications\nExpected Carbon Credits: 25,000 tonnes of CO₂ equivalent annually\nCarbon Credit Standard: Verified Carbon Standard (VCS)\nCarbon Credit Methodology: VCS Methodology VM0017 (Afforestation and Reforestation)\nVerification Status: Not yet verified; expected verification by Q2 of 2025.\n4. Financial & Pricing Information\nTotal Project Budget: $2,500,000\nSources of Funding:\nInternal Funding: $1,000,000\nExternal Investors: $1,200,000\nGrant Support: $300,000 (from the Green Climate Fund)\nCarbon Credit Pricing: $15 per tonne of CO₂ equivalent\n5. Project Impact and Sustainability\nEnvironmental Impact:\nThe project is expected to sequester approximately 100,000 tonnes of CO₂ over its lifetime while restoring native habitats and increasing biodiversity by planting over 200,000 indigenous trees.\nCommunity and Social Benefits:\nThe initiative will create around 150 jobs in tree planting and maintenance, provide training in sustainable agriculture to local farmers, and enhance community infrastructure through improved access roads.\nSustainability and Long-term Goals:\nThe project will establish a community stewardship program ensuring ongoing maintenance and monitoring of the forest. Plans for future expansion include additional reforestation sites and eco-tourism initiatives.\n6. Risks & Mitigation Strategies\nRisk Assessment:\nKey risks include deforestation due to illegal logging, climate change impacts on growth rates, and financial instability.\nMitigation Strategies:\nImplementing strict monitoring protocols, engaging local communities in protection efforts, securing insurance against natural disasters, and diversifying funding sources.\n7. Supporting Documentation\nProject Plan: Attached (PDF)\nEnvironmental Impact Assessment (EIA): Attached (PDF)\nVerification Reports: Not applicable at this stage.\nFinancial Projections: Attached (PDF)\nAdditional Certifications or Approvals: Attached (PDF)\n8. Declarations and Acknowledgements\nBy signing below, I certify that the information provided is accurate and complete to the best of my knowledge. I acknowledge that any false or misleading information may result in the disqualification of this submission.\nName of Authorized Signatory: Carlos Mendes\nSignature: [Digital Signature]\nDate: 10/06/2024\nSubmission Guidelines:\nReview and Complete: Ensure all sections are fully completed and accurate.\nAttach Documents: Attach all required supporting documentation in the appropriate format (PDF preferred).\nSubmit: Submit the completed form and attachments via email to submissions@ecosustainability.org or through the Online Submission Portal.\n\"\"\"\ngenerator.generate_document(input_text, '/kaggle/working/carbon_credit_document2.docx')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-06T00:00:26.052683Z","iopub.execute_input":"2024-10-06T00:00:26.053165Z","iopub.status.idle":"2024-10-06T00:04:51.300001Z","shell.execute_reply.started":"2024-10-06T00:00:26.053122Z","shell.execute_reply":"2024-10-06T00:04:51.298652Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5fb814a82734bb4a2213f059b03b261"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d7a664adff9411fa56802714958e4a8"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7277667a3bf4924ad237a7d083f4704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a8d88f809da4f0691af657103417c17"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f10355b1f5749bb8cf9a6e3e2573342"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0968b7e1fd604faeb752bdb200c8204e"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f32321844404a37bcbe867b6c05ed75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f399032b4047416abcad3882a9316793"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bb312e541754d89aa67f8a4c741710f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d6015fe1b9a426f8510edcdb4d0d1ff"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dbb683c84fb46119cddbbe1ec333b21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92debddd195042d69dae0cea80dd46c1"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4a993d2cad9400eb84cd6df5b0ee8c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e282210ab5c043919070354375bec3f4"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef05deee8131433f992f7398c133a8b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ac61a7895c845a3b88ec4bee4336052"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Carbon credit document generated and saved to /kaggle/working/carbon_credit_document2.docx\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Using Sbert model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom docx import Document\n\nclass SBERTCarbonCreditDocGenerator:\n    def __init__(self):\n        self.sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.knowledge_base = self.load_knowledge_base()\n        self.document_structure = self.load_document_structure()\n        \n    def load_knowledge_base(self):\n        # This should load your carbon credit domain knowledge\n        return [\n            \"Carbon credits represent the reduction of one metric ton of carbon dioxide emissions.\",\n            \"Afforestation projects involve planting trees in areas where there were none before.\",\n            \"The Verified Carbon Standard (VCS) is a widely recognized certification for carbon credits.\",\n            \"Carbon credit projects must demonstrate additionality, meaning the reductions wouldn't have occurred without the project.\",\n            \"Monitoring, reporting, and verification (MRV) are crucial components of carbon credit projects.\",\n            \"Project developers must provide detailed information about the project location, type, and expected carbon sequestration.\",\n            \"The price of carbon credits can vary based on project type, location, and additional benefits like biodiversity conservation.\",\n            \"Carbon credit projects often have social and environmental co-benefits beyond carbon sequestration.\",\n            \"The crediting period for forestry projects typically ranges from 20 to 100 years.\",\n            \"Leakage, or the displacement of emissions to areas outside the project boundary, must be accounted for in carbon credit calculations.\",\n        ]\n        \n    def load_document_structure(self):\n        return [\n            \"Executive Summary\",\n            \"Certificate Identification\",\n            \"Emission Reduction Details\",\n            \"Project Information\",\n            \"Verification and Certification\",\n            \"Issuance and Expiration Dates\",\n            \"Market Type\",\n            \"Transferability Information\",\n            \"Legal Framework\",\n            \"Accountability Measures\",\n            \"Contact Information\"\n        ]\n        \n    def process_input_data(self, input_text):\n        lines = input_text.split('\\n')\n        data = {}\n        for line in lines:\n            if ':' in line:\n                key, value = line.split(':', 1)\n                data[key.strip()] = value.strip()\n        return data\n    \n    def retrieve_relevant_knowledge(self, query, top_k=3):\n        query_embedding = self.sbert_model.encode([query])[0]\n        knowledge_embeddings = self.sbert_model.encode(self.knowledge_base)\n        \n        similarities = cosine_similarity([query_embedding], knowledge_embeddings)[0]\n        top_indices = np.argsort(similarities)[-top_k:][::-1]\n        \n        return [self.knowledge_base[i] for i in top_indices]\n    \n    def generate_section_content(self, section_title, input_data):\n        relevant_input = self.get_relevant_input(section_title, input_data)\n        relevant_knowledge = self.retrieve_relevant_knowledge(section_title)\n        \n        content = f\"{section_title}:\\n\\n\"\n        content += \"Project-specific information:\\n\"\n        for key, value in relevant_input.items():\n            content += f\"- {key}: {value}\\n\"\n        \n        content += \"\\nRelevant domain knowledge:\\n\"\n        for knowledge in relevant_knowledge:\n            content += f\"- {knowledge}\\n\"\n        \n        return content\n    \n    def get_relevant_input(self, section_title, input_data):\n        section_embedding = self.sbert_model.encode([section_title])[0]\n        input_embeddings = self.sbert_model.encode(list(input_data.keys()))\n        \n        similarities = cosine_similarity([section_embedding], input_embeddings)[0]\n        relevant_indices = np.where(similarities > 0.3)[0]  # Adjust threshold as needed\n        \n        return {k: input_data[k] for i, k in enumerate(input_data.keys()) if i in relevant_indices}\n    \n    def create_document(self, input_text):\n        doc = Document()\n        doc.add_heading('Carbon Credit Project Document', 0)\n        \n        input_data = self.process_input_data(input_text)\n        \n        for section in self.document_structure:\n            doc.add_heading(section, level=1)\n            content = self.generate_section_content(section, input_data)\n            doc.add_paragraph(content)\n        \n        return doc\n    \n    def generate_document(self, input_text, output_path):\n        doc = self.create_document(input_text)\n        doc.save(output_path)\n        print(f\"Carbon credit document generated and saved to {output_path}\")\n\n# Usage\ngenerator = SBERTCarbonCreditDocGenerator()\ninput_text = \"\"\"\nCarbon Credit Project Submission Form\n1. Project Overview\nProject Title: Green Future Reforestation Initiative\nExecutive Summary:\nThe Green Future Reforestation Initiative aims to restore 500 hectares of degraded land through afforestation and sustainable land management practices. The project seeks to sequester carbon dioxide, enhance biodiversity, and improve local air quality while providing economic opportunities for surrounding communities.\nLocation:\nCountry: Brazil\nRegion: Amazon Rainforest\nCoordinates: Latitude -3.4653, Longitude -62.2159\nProject Category: Afforestation\nProject Start Date: 01/15/2024\nExpected Completion Date: 12/31/2028\n2. Seller/Proponent Information\nOrganization Name: EcoSustainability Inc.\nContact Person: Maria Silva\nJob Title: Project Manager\nEmail Address: maria.silva@ecosustainability.org\nPhone Number: +55 11 98765-4321\nMailing Address:\nEcoSustainability Inc.\nAvenida Paulista, 1000\nSão Paulo, SP, Brazil\nZip Code: 01310-100\n3. Carbon Credit Specifications\nExpected Carbon Credits: 25,000 tonnes of CO₂ equivalent annually\nCarbon Credit Standard: Verified Carbon Standard (VCS)\nCarbon Credit Methodology: VCS Methodology VM0017 (Afforestation and Reforestation)\nVerification Status: Not yet verified; expected verification by Q2 of 2025.\n4. Financial & Pricing Information\nTotal Project Budget: $2,500,000\nSources of Funding:\nInternal Funding: $1,000,000\nExternal Investors: $1,200,000\nGrant Support: $300,000 (from the Green Climate Fund)\nCarbon Credit Pricing: $15 per tonne of CO₂ equivalent\n5. Project Impact and Sustainability\nEnvironmental Impact:\nThe project is expected to sequester approximately 100,000 tonnes of CO₂ over its lifetime while restoring native habitats and increasing biodiversity by planting over 200,000 indigenous trees.\nCommunity and Social Benefits:\nThe initiative will create around 150 jobs in tree planting and maintenance, provide training in sustainable agriculture to local farmers, and enhance community infrastructure through improved access roads.\nSustainability and Long-term Goals:\nThe project will establish a community stewardship program ensuring ongoing maintenance and monitoring of the forest. Plans for future expansion include additional reforestation sites and eco-tourism initiatives.\n6. Risks & Mitigation Strategies\nRisk Assessment:\nKey risks include deforestation due to illegal logging, climate change impacts on growth rates, and financial instability.\nMitigation Strategies:\nImplementing strict monitoring protocols, engaging local communities in protection efforts, securing insurance against natural disasters, and diversifying funding sources.\n7. Supporting Documentation\nProject Plan: Attached (PDF)\nEnvironmental Impact Assessment (EIA): Attached (PDF)\nVerification Reports: Not applicable at this stage.\nFinancial Projections: Attached (PDF)\nAdditional Certifications or Approvals: Attached (PDF)\n8. Declarations and Acknowledgements\nBy signing below, I certify that the information provided is accurate and complete to the best of my knowledge. I acknowledge that any false or misleading information may result in the disqualification of this submission.\nName of Authorized Signatory: Carlos Mendes\nSignature: [Digital Signature]\nDate: 10/06/2024\nSubmission Guidelines:\nReview and Complete: Ensure all sections are fully completed and accurate.\nAttach Documents: Attach all required supporting documentation in the appropriate format (PDF preferred).\nSubmit: Submit the completed form and attachments via email to submissions@ecosustainability.org or through the Online Submission Portal.\n\"\"\"\ngenerator.generate_document(input_text, '/kaggle/working/carbon_credit_document_sbert2.docx')","metadata":{"execution":{"iopub.status.busy":"2024-10-05T23:57:12.672504Z","iopub.execute_input":"2024-10-05T23:57:12.673121Z","iopub.status.idle":"2024-10-05T23:57:17.969989Z","shell.execute_reply.started":"2024-10-05T23:57:12.673052Z","shell.execute_reply":"2024-10-05T23:57:17.968739Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6219f0334011442da13cda3f61d941cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6de97a71b8ae45ff9b5fff6c10fd9480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a96e275a78a4215a9f35acdba119a58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e0548a7c19d4ba9adcfbb9411d6afd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d7ad5ec9f9140c389b2985572a30915"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5679fbd02a344b4cb21f7f90a0d4258f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef751a9aa9654ffab7a1f9a8414af7c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98909bd2cfc54b308e722430dca212d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8bc2aa69da34b1b8efb2db0bd2c026c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd882d6b68b41ac8fae6405f42f153b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc5b389caa8e42938ab1f836fea59a9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc92737e12304f229a04147d665e25aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"300a01a4c58e445797f65d3fe2c87e01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d830a6fc0a247e8a551324c91f7651b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8c85368741f4e66bcd4a363be9ca276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3fb5755d03b4746b6410020c045f8e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef70a65fda0045f0a6a77a2a562fa8bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"301b2c8809bc4974b1a435f6da11a758"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec9840f50e0643a58ab383686c913843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63c5c80678a24a5eb1485475384db0f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dc49de01582445e9dbc9d68ef41ee87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acf614aac4d440a28987239014da5284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f354133ad87f4a83a5d8742681811520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5754ba5397dc4c5f86c37a7855bd4c39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99090411ad6340818dd2dd9583c9f5e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67db658ed20744a4861ce5ad0122e3e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dfaddaeed8e4d648e77af3019b49626"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"941d3faf64f84e79a659641aa63600e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90d9b232394641e4af62905e1c020825"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec495580be6e40759e5e20745b43aa4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c68234108646f2afe80744b73be330"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baae8eb1d3064232bf7ceb19a8d08185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07acee97044446d18282051788e9f252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3142390403fa4761ab024cc5552bc3be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a09c1fdd1014493afea6355ab7e5a74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e57facc41114624a7e70ea3bc6ba82c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"822dbfd12c5642fdb989ca053b954744"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2257278c1fe447daa6a4ab1f24b7d98f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"018b1d36d22b49aa976c38af4d85e8f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e89f66cf5914e1792385209288a1c0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d0d978ad20480bb1bebed7cd0241ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4f7817cefb94590ad47202818d293e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43eea19da48446a58c22986dbb7ce013"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c74adce56bcc4f7db8303d5f5021f574"}},"metadata":{}},{"name":"stdout","text":"Carbon credit document generated and saved to /kaggle/working/carbon_credit_document_sbert2.docx\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}